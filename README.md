# Resume Classification NLP System

A production-ready application that automatically categorizes resumes into job categories using Natural Language Processing and Machine Learning techniques.

## ğŸ¯ Project Overview

This system processes PDF resumes, extracts and cleans text content, applies machine learning models for classification, and provides a user-friendly web interface for batch processing with automated file organization and results export.

### Key Features

- **Automated Resume Classification**: Categorizes resumes into 25 predefined job categories
- **High Accuracy**: Achieves 90%+ classification accuracy using multiple ML algorithms
- **Web Interface**: Streamlit-based UI with drag-and-drop file upload
- **Batch Processing**: Handle multiple resumes simultaneously with real-time progress tracking
- **Automated Organization**: Automatically organizes classified resumes into category folders
- **Export Functionality**: Export results to CSV format with confidence scores
- **Comprehensive Testing**: Unit and integration tests with >80% code coverage

## ğŸ—ï¸ Project Structure

```
resume-classification-nlp/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Original datasets
â”‚   â”œâ”€â”€ processed/              # Cleaned data
â”‚   â””â”€â”€ test_resumes/           # Sample PDFs
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl    # Trained vectorizer
â”‚   â”œâ”€â”€ best_model.pkl          # Best performing model
â”‚   â””â”€â”€ category_mapping.pkl    # Label mappings
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_preprocessing.py   # Text cleaning functions
â”‚   â”œâ”€â”€ feature_engineering.py # TF-IDF implementation
â”‚   â”œâ”€â”€ model_training.py       # ML pipeline
â”‚   â”œâ”€â”€ web_app.py             # Streamlit interface
â”‚   â”œâ”€â”€ utils.py               # Helper functions
â”‚   â””â”€â”€ logger_setup.py        # Logging configuration
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ exploratory_analysis.ipynb
â”‚   â”œâ”€â”€ model_development.ipynb
â”‚   â””â”€â”€ performance_evaluation.ipynb
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_web_app.py
â”œâ”€â”€ categorized_resumes/        # Output directory
â”œâ”€â”€ logs/                       # Application logs
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config.py                   # Configuration settings
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

## ğŸ¯ Target Categories

The system classifies resumes into 25 job categories:

- Data Scientist
- Java Developer
- Python Developer
- Web Developer
- Business Analyst
- HR
- DevOps Engineer
- Software Engineer
- Testing
- Network Security Engineer
- SAP Developer
- Hardware
- Automation Testing
- Electrical Engineering
- Operations Manager
- PMO
- Database
- Hadoop
- ETL Developer
- DotNet Developer
- Blockchain
- Sales
- Mechanical Engineer
- Civil Engineer
- Arts

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Installation

1. Clone the repository:

```bash
git clone <repository-url>
cd resume-classification-nlp
```

2. Create a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:

```bash
pip install -r requirements.txt
```

4. Download NLTK data:

```python
import nltk
nltk.download('stopwords')
nltk.download('punkt')
```

5. Initialize the project structure:

```bash
python config.py
```

### Running the Application

#### Option 1: Using the startup script (Recommended)

```bash
python run_web_app.py
```

#### Option 2: Direct Streamlit command

```bash
streamlit run src/web_app.py
```

The application will automatically:

- Validate configuration settings
- Create necessary directories
- Check for trained models
- Start the web interface at `http://localhost:8501`

### Using the Web Interface

1. **Upload Files**: Drag and drop PDF resumes or use the file browser
2. **Configure Options**: Choose file organization and export settings
3. **Start Processing**: Click "Start Classification" to begin
4. **View Results**: Check the "Results & Analytics" tab for detailed results
5. **Export Data**: Download CSV reports or summary documents
6. **Organize Files**: Automatically sort resumes by predicted category

## ğŸ”§ Configuration

The system is highly configurable through `config.py`. Key settings include:

- **TF-IDF Parameters**: Max features (5000), n-gram range (1,2)
- **ML Models**: KNN, Logistic Regression, Random Forest, SVM, Naive Bayes
- **Performance Requirements**: 90% accuracy, <5s per resume processing
- **File Handling**: Supported formats, batch sizes, memory limits

## ğŸ§ª Testing

Run the test suite:

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test file
pytest tests/test_preprocessing.py
```

## ğŸ“Š Performance Metrics

- **Classification Accuracy**: >90%
- **Processing Speed**: <5 seconds per resume
- **Memory Usage**: <2GB for batch processing
- **Error Rate**: <5% on diverse resume formats
- **Test Coverage**: >80%

## ğŸ› ï¸ Development

### Code Quality

The project follows strict code quality standards:

- **PEP 8 Compliance**: All Python code follows PEP 8 standards
- **Type Hints**: Comprehensive type annotations
- **Documentation**: Detailed docstrings for all functions
- **Error Handling**: Comprehensive error management
- **Logging**: Detailed logging for debugging and monitoring

### Adding New Categories

To add new job categories:

1. Update `CATEGORY_MAPPING` in `config.py`
2. Retrain models with new category data
3. Update tests and documentation

## ğŸ“ˆ Model Performance

The system compares multiple ML algorithms and automatically selects the best performer:

- **K-Nearest Neighbors (KNN)**
- **Logistic Regression** with L2 regularization
- **Random Forest** with feature importance
- **Support Vector Machine (SVM)** with RBF kernel
- **Multinomial Naive Bayes**

Performance is evaluated using:

- Accuracy, Precision, Recall, F1-score
- Confusion matrix analysis
- Cross-validation scores
- Feature importance analysis

## ğŸ”’ Security Considerations

- File type validation for uploads
- Memory management for large files
- No persistent storage of resume content
- Secure CSV export functionality
- Input sanitization and validation

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ Support

For support and questions:

- Create an issue in the repository
- Check the documentation in the `docs/` directory
- Review the test files for usage examples

## ğŸ“ Learning Outcomes

This project demonstrates:

- Advanced NLP text preprocessing techniques
- Machine learning model comparison and selection
- Web application development with Streamlit
- Data pipeline creation and optimization
- Model persistence and deployment
- Production-ready code organization
- Comprehensive testing strategies

Perfect for showcasing in technical interviews and professional portfolios!
